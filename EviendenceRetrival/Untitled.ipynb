{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0402396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpr_setting(args):\n",
    "    # default setting\n",
    "    args.batch_size = getattr(args, 'batch_size', 8)\n",
    "    args.epoch = getattr(args, 'epoch', 20)\n",
    "    args.report_freq = getattr(args, \"report_freq\", 5)\n",
    "    args.accumulate_step = getattr(args, \"accumulate_step\", 2) #梯度累加，batch size很小的时候用\n",
    "    args.model_type = getattr(args, \"model_type\", \"princeton-nlp/sup-simcse-roberta-base\")\n",
    "    args.warmup_steps = getattr(args, \"warmup_steps\", 200)\n",
    "    args.grad_norm = getattr(args, \"grad_norm\", 1)\n",
    "    args.seed = getattr(args, \"seed\", 42)\n",
    "    args.max_lr = getattr(args, \"max_lr\", 2e-5)\n",
    "    args.max_length = getattr(args, \"max_length\", 128)\n",
    "    args.eval_interval = getattr(args, \"eval_interval\", 20) #评估次数\n",
    "    args.retrieval_num = getattr(args, \"retrieval_num\", 4)\n",
    "    args.evidence_samples = getattr(args, \"evidence_samples\", 64) #一个batch里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be9987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb59644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def to_cuda(batch):\n",
    "    for n in batch.keys():\n",
    "        if n in [\"query_input_ids\", \"evidence_input_ids\", \"query_attention_mask\", \"evidence_attention_mask\"]:\n",
    "            batch[n] = batch[n].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9acc7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, mode, tok, evidence_samples, max_length=512, using_negative=True):\n",
    "        self.max_length = max_length\n",
    "        if using_negative:\n",
    "            f = open(\"data/train-claims-with-negatives.json\", \"r\") #生成负样本，造数据。\n",
    "        else:\n",
    "            f = open(\"data/{}-claims.json\".format(mode), \"r\")\n",
    "        self.dataset = json.load(f)\n",
    "        f.close()\n",
    "        self.using_negative = using_negative\n",
    "        # f = open(\"data/evidence.json\", \"r\")\n",
    "        f = open(\"temp_data/reduced-evidences.json\", \"r\")\n",
    "        self.evidences = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        self.tokenizer = tok\n",
    "        self.claim_ids = list(self.dataset.keys())\n",
    "        self.mode = mode\n",
    "        self.evidence_samples = evidence_samples\n",
    "        self.evidence_ids = list(self.evidences.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = self.dataset[self.claim_ids[idx]]\n",
    "        processed_query = process(data[\"claim_text\"])\n",
    "        evidences = []\n",
    "        for evidence_id in data[\"evidences\"]:\n",
    "            evidences.append(evidence_id)\n",
    "        if self.using_negative:\n",
    "            negative_evidences = data[\"negative_evidences\"]\n",
    "            return [processed_query, evidences, negative_evidences]\n",
    "        else:\n",
    "            return [processed_query, evidences]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        queries = []\n",
    "        evidences = []\n",
    "        labels = []\n",
    "        if self.using_negative:\n",
    "            negative_evidences = []\n",
    "            for query, evidence, negative_evidence in batch:\n",
    "                queries.append(query)\n",
    "                evidences.extend(evidence)\n",
    "                negative_evidences.extend(negative_evidence)\n",
    "                labels.append(len(evidence))\n",
    "            evidences.extend(negative_evidences)\n",
    "        else:\n",
    "            for query, evidence in batch:\n",
    "                queries.append(query)\n",
    "                evidences.extend(evidence)\n",
    "                labels.append(len(evidence))\n",
    "        cnt = len(evidences)\n",
    "        if cnt > self.evidence_samples:\n",
    "            evidences = evidences[:self.evidence_samples]\n",
    "        evidences_text = [process(self.evidences[evidence_id]) for evidence_id in evidences]\n",
    "        while cnt < self.evidence_samples: #继续做负采样\n",
    "            evidence_id = random.choice(self.evidence_ids)\n",
    "            while evidence_id in evidences:\n",
    "                evidence_id = random.choice(self.evidence_ids)\n",
    "            evidences.append(evidence_id)\n",
    "            evidences_text.append(process(self.evidences[evidence_id]))\n",
    "            cnt += 1\n",
    "\n",
    "        query_text = self.tokenizer(\n",
    "            queries,\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        evidence_text = self.tokenizer(\n",
    "            evidences_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        batch_encoding = dict()\n",
    "        batch_encoding[\"query_input_ids\"] = query_text.input_ids\n",
    "        batch_encoding[\"evidence_input_ids\"] = evidence_text.input_ids\n",
    "        batch_encoding[\"query_attention_mask\"] = query_text.attention_mask\n",
    "        batch_encoding[\"evidence_attention_mask\"] = evidence_text.attention_mask\n",
    "        batch_encoding[\"labels\"] = labels\n",
    "        return batch_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5425469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidenceDataset(Dataset):\n",
    "    def __init__(self, tok, max_length=512):\n",
    "        self.max_length = max_length\n",
    "\n",
    "        f = open(\"temp_data/reduced-evidences.json\", \"r\")\n",
    "        # f = open(\"data/evidence.json\", \"r\")\n",
    "        self.evidences = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        self.tokenizer = tok\n",
    "        self.evidences_ids = list(self.evidences.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.evidences_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        evidences_id = self.evidences_ids[idx]\n",
    "        evidence = self.evidences[evidences_id]\n",
    "        return [evidences_id, evidence]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        evidences_ids = []\n",
    "        evidences = []\n",
    "\n",
    "        for evidences_id, evidence in batch:\n",
    "            evidences_ids.append(evidences_id)\n",
    "            evidences.append(process(evidence))\n",
    "\n",
    "        evidences_text = self.tokenizer(\n",
    "            evidences,\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        batch_encoding = dict()\n",
    "        batch_encoding[\"evidence_input_ids\"] = evidences_text.input_ids\n",
    "        batch_encoding[\"evidence_attention_mask\"] = evidences_text.attention_mask\n",
    "        batch_encoding[\"evidences_ids\"] = evidences_ids\n",
    "        return batch_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47c3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(Dataset):\n",
    "    def __init__(self, mode, tok, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        if mode != \"test\":\n",
    "            f = open(\"data/{}-claims.json\".format(mode), \"r\")\n",
    "        else:\n",
    "            f = open(\"data/test-claims-unlabelled.json\", \"r\")\n",
    "        self.dataset = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        self.tokenizer = tok\n",
    "        self.claim_ids = list(self.dataset.keys())\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[self.claim_ids[idx]]\n",
    "        processed_text = process(data[\"claim_text\"])\n",
    "        return [processed_text, data, self.claim_ids[idx]]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        queries = []\n",
    "        datas = []\n",
    "        evidences = []\n",
    "        claim_ids = []\n",
    "        for query, data, claim_id in batch:\n",
    "            queries.append(query)\n",
    "            datas.append(data)\n",
    "            if self.mode != \"test\":\n",
    "                evidences.append(data[\"evidences\"])\n",
    "            claim_ids.append(claim_id)\n",
    "\n",
    "        query_text = self.tokenizer(\n",
    "            queries,\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        batch_encoding = dict()\n",
    "        batch_encoding[\"query_input_ids\"] = query_text.input_ids\n",
    "        batch_encoding[\"query_attention_mask\"] = query_text.attention_mask\n",
    "\n",
    "        batch_encoding[\"datas\"] = datas\n",
    "        batch_encoding[\"claim_ids\"] = claim_ids\n",
    "        if self.mode != \"test\":\n",
    "            batch_encoding[\"evidences\"] = evidences\n",
    "        return batch_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c295d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
